# Full Prompt for Quiz & Exercise Use Case Implementation (UC3.4 – Extended)

## Role & Responsibility

You are a **Senior Software Engineer** specializing in **full-stack development** and **database-driven system design**.

Your task is to generate a **complete technical specification** for implementing **Quiz / Exercise Submission and Scoring** within a **JLPT learning platform**, including:

* System-defined quizzes
* **Custom quizzes/exercises generated by AI chatbot**
* **Manually created quizzes/exercises** (by teachers or users)

⚠️ **Important Constraint**
You must define **logic responsibilities, data flow, and interaction contracts only**.
All **codebase-specific decisions** (schemas, class names, method names, controller boundaries, DTOs) **must be decided by the AI Code Agent**.

---

## 1. Context and Goal (Codebase & Architecture)

### Target Feature

**Quiz / Exercise Submission, Scoring, and Learning Impact Processing**

### Architecture (Conceptual)

```
Frontend (e.g. Next.js)
   ↓
API Layer (e.g. Express / HTTP Interface)
   ↓
Core Domain Services
   ↓
Persistence Layer (MongoDB or equivalent)
```

⚠️ Exact frameworks, folder structure, and service boundaries are **intentionally undefined** and must be decided by the Code Agent.

---

### Primary Goal

Define the **required behavior and data interactions** to:

* Validate submissions
* Score quizzes or exercises
* Persist attempt results
* Update **Spaced Repetition System (SRS)** data
* Trigger **AI Feedback** generation

This must work consistently for:

1. Predefined system quizzes
2. Chatbot-generated quizzes/exercises
3. Manually created quizzes/exercises

---

### Interacting Logic Systems

* Submission & Scoring Logic
* SRS Logic System
* AI Feedback Logic System
* (Optional) Authoring / Generation Metadata Logic

---

## 2. Quiz / Exercise Source Types (Critical Extension)

The system must support **multiple quiz origins**, which affect validation and scoring behavior.

### 2.1 Quiz / Exercise Origins

#### A. System-Defined Quizzes

* Pre-authored
* Stored permanently
* Immutable content
* Authoritative correct answers

#### B. Chatbot-Generated Quizzes / Exercises

* Generated dynamically per user or session
* May be:

  * Fully AI-authored
  * AI-assembled from existing learning items
* Correct answers may be:

  * Explicitly generated
  * Derived from referenced SRS flashcards

#### C. Manually Created Quizzes / Exercises

* Created by:

  * Teachers
  * Advanced users
* Can be reused or private
* May include mixed question types

⚠️ The system must **not assume** that all quizzes exist as permanent, static documents.

---

## 3. Database Interactions & Dependencies (Conceptual)

Define **what must be read or written**, not exact collections or schemas.

---

### 3.1 Quiz / Exercise Content Dependency (Read)

For **system quizzes**, content is retrieved directly.

For **custom quizzes (AI or manual)**:

* Content may be:

  * Stored temporarily
  * Stored permanently
  * Embedded in submission payload
* Correct answer definitions may come from:

  * Quiz content
  * Linked SRS flashcards
  * AI-generated answer keys

#### Required High-Level Capabilities

* Identify question type
* Identify scoring rules
* Identify linked learning items (SRS Flashcards)
* Distinguish quiz origin (system / chatbot / manual)

---

### 3.2 Supported JLPT Question Types

The scoring logic must dynamically adapt to:

#### Grammar

* Fill-in-the-blank
* Sentence assembly (ordering)

#### Vocabulary / Kanji

* Reading selection
* Synonym selection

#### Reading Comprehension

* Questions linked to one passage
* One question may affect multiple learning items

---

### 3.3 Quiz / Exercise Result Storage (Write)

Each submission must produce a **single attempt record**.

#### Required Stored Information

* User identifier
* Quiz/exercise identifier (or generated ID)
* Quiz origin type
* Completion timestamp
* Final score
* Per-question results:

  * Question reference or index
  * User answer
  * Scoring outcome
  * Partial score (if applicable)
* List of weak / missed learning points:

  * Linked SRS Flashcard ID(s)
  * Question type
  * Origin (system / AI / manual)

---

### 3.4 SRS Data Interaction (Read / Write)

The system must:

* Read current learning state
* Update learning parameters based on:

  * Correctness
  * Question type
  * Quiz origin
  * Context (review vs test vs practice)

⚠️ SRS rules may differ depending on whether the quiz is:

* Formal assessment
* Practice exercise
* AI-generated reinforcement

---

## 4. Core Logic Flow (High-Level Task Sequence)

This flow applies to **all quiz origins**, with adaptive behavior.

---

### Step 1: Submission Validation

* Validate structural integrity
* Confirm scoring references exist
* Ensure question count consistency
* Handle missing or dynamic answer keys gracefully

---

### Step 2: Scoring Operation

* Determine quiz origin
* Load or resolve authoritative answers
* Apply scoring rules per question type:

  * Partial credit where applicable
  * Binary scoring where required
* Aggregate final score
* Identify weak learning points

---

### Step 3: Result Persistence

* Store detailed attempt data
* Preserve traceability for:

  * AI feedback
  * SRS updates
  * User progress tracking

---

### Step 4: SRS Adjustment

* Map incorrect or weak answers to flashcards
* Apply learning algorithm updates
* Support:

  * One question → many flashcards
  * Many questions → one flashcard
* Allow quiz-origin-specific weighting rules

---

### Step 5: AI Feedback Trigger

* Send structured summary to AI Feedback system
* Include:

  * Quiz origin
  * Question types
  * Weak areas
  * JLPT level context

---

## 5. Critical Backend Operations (Abstract Definitions)

---

### 5.1 Main Submission Orchestrator

**Purpose**

* Central coordinator for submission handling

**Responsibilities**

* Input validation
* Scoring orchestration
* Persistence triggering
* SRS update invocation
* AI feedback request

**Inputs**

* User identifier
* Quiz/exercise payload
* Origin metadata
* User answers

**Outputs**

* Submission status
* Final score
* Summary response for frontend

---

### 5.2 SRS Adjustment Logic

**Purpose**

* Translate quiz performance into learning updates

**Responsibilities**

* Interpret correctness signals
* Apply algorithmic learning rules
* Support batch and multi-entity updates

**Inputs**

* User identifier
* Flashcard references
* Performance indicators
* Quiz origin and context

**Outputs**

* Updated learning state data

---

## 6. Design & Testing Expectations

The implementation must:

* Support dynamic quiz definitions
* Avoid tight coupling to static quiz schemas
* Be extensible for future question types
* Ensure deterministic scoring
* Cleanly separate:

  * Scoring
  * Persistence
  * Learning logic
* Handle AI-generated content safely and consistently

---

**End of Prompt**
